{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, Trainer, RobertaTokenizer, LongformerForSequenceClassification\n",
    "from torchmetrics import F1, Recall, Precision, PrecisionRecallCurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/u9/kbozler/Documents/outside/reuters-article-classification/data/'\n",
    "output_path = '/xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel'\n",
    "model_save_dir = os.path.join('/xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/saved-models/longformer-reuters-multilabel')\n",
    "all_topics = ['earn', 'acq', 'money-fx', 'grain', 'crude', 'trade', 'interest']\n",
    "num_labels = len(all_topics)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = get_filenames(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reut2-015.sgm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u9/kbozler/Documents/outside/reuters-article-classification/helpers.py:29: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 29 of the file /home/u9/kbozler/Documents/outside/reuters-article-classification/helpers.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reut2-008.sgm\n",
      "reut2-006.sgm\n",
      "reut2-010.sgm\n",
      "reut2-009.sgm\n",
      "reut2-005.sgm\n",
      "reut2-003.sgm\n",
      "reut2-019.sgm\n",
      "reut2-011.sgm\n",
      "reut2-016.sgm\n",
      "reut2-007.sgm\n",
      "reut2-004.sgm\n",
      "reut2-012.sgm\n",
      "reut2-013.sgm\n",
      "reut2-018.sgm\n",
      "reut2-002.sgm\n",
      "reut2-017.sgm\n",
      "reut2-014.sgm\n",
      "reut2-000.sgm\n",
      "reut2-020.sgm\n",
      "reut2-001.sgm\n"
     ]
    }
   ],
   "source": [
    "dataset_train_dict, dataset_val_dict, dataset_test_dict = build_dataset_dictionaries(dataset_path, \n",
    "                                                                                     filenames, \n",
    "                                                                                     all_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_train, csv_eval, csv_test = write_to_csv(dataset_path, \n",
    "                                             all_topics, \n",
    "                                             dataset_train_dict, \n",
    "                                             dataset_val_dict, \n",
    "                                             dataset_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2e9295c49fb71df0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/u9/kbozler/.cache/huggingface/datasets/csv/default-2e9295c49fb71df0/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde5f319d43846088d6aaa78baca36d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d63e118cccb40deab2cddf31c38d966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/u9/kbozler/.cache/huggingface/datasets/csv/default-2e9295c49fb71df0/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66b12398d114eb4ba4b2fc037db7396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'text', 'earn', 'acq', 'money-fx', 'grain', 'crude', 'trade', 'interest'],\n",
       "        num_rows: 12668\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['idx', 'text', 'earn', 'acq', 'money-fx', 'grain', 'crude', 'trade', 'interest'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'text', 'earn', 'acq', 'money-fx', 'grain', 'crude', 'trade', 'interest'],\n",
       "        num_rows: 5610\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset('csv', data_files={'train': os.path.join(dataset_path, csv_train),\n",
    "                                               'eval': os.path.join(dataset_path, csv_eval),\n",
    "                                               'test': os.path.join(dataset_path, csv_test)})\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD5CAYAAAAp8/5SAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAaMUlEQVR4nO3dfbQdVZ3m8e8DRCG8GE2iolFD2zTK0BAyaUDSIgK6CCC+NM6CER1dKM3ICDgyOtqOwJrpWTjj0NIv0sbwYreYVhKwaWwZ0mBABEJeSEhCwBcIGJo2QYEQcNDAb/7Y+8Ti5tx76p5Tu25y+/msdVfOObX3rV+dU6lbt249eysiMDOz8WuXsS7AzMzK8oHezGyc84HezGyc84HezGyc84HezGyc84HezGyc261OI0nHA5cCuwLzIuLiIcs/AHwmP90C/MeIWFVZviuwDHg0Ik7qtb4pU6bE9OnTa22AmZnB8uXLH4+Iqd2W9TzQ54P0XwHvADYASyVdHxH3VZo9BLwtIp6QNAeYCxxeWX4usA7Yp07B06dPZ9myZXWampkZIOnh4ZbVuXRzGPCTiHgwIn4N/B3w7mqDiLgjIp7IT+8CplVWPg04EZg32sLNzGxwdQ70rwV+Vnm+Ib82nDOA71Wefxn4NPDCSCuRdKakZZKWbdq0qUZZZmZWR50Dvbq81nXcBElvJx3oP5OfnwRsjIjlvVYSEXMjYlZEzJo6tetlJjMz60OdP8ZuAF5XeT4N+OehjSQdTLo8MycifpFfng2cLOkEYHdgH0nfiIjTByvbzMzqqnNGvxTYX9J+kl4CnApcX20g6fXAtcAHI+JHndcj4rMRMS0ipud+t/ggb2bWrp5n9BGxVdJ/Av4v6fbKKyJiraSz8vK/Br4ATAa+Iglga0TMKle2mZnVpR1xmOJZs2aFb680M6tP0vLhTrBrJWMlHS/pAUk/kfRfuyz/gKR789cdkg7Jr+8u6W5JqyStlXTRYJtiZmajVTow9RxwTERskTQBuF3S9yLirsa3xMzMuqpz1822wBSApE5gatuBPiLuqLTfFpiKdF1oS359Qv4qeq1IF3W7G7QZccGOd5nLzKyX4oEpSbtKWglsBBZFxJJunRyYMjMro2hgCiAino+IGaSz/MMkHdStrwNTZmZl1DnQjzYw9e5KYGqbiHgSWAwc30+hZmbWn6KBKUlTJU3Kj/cAjgPub6h2MzOroXRgal/g6/nOnV2Ab0fEDWU2xczMuqk18UhE/CPwj0Ne++vK448CH+3S717g0AFrNDOzAZQOTL1O0vclrcuBqXOb3gAzMxtZ6cDUVuBTEbFC0t7AckmLhvQ1M7OCis4wFRGPRcSK/Php0nSCI92Db2ZmDWtjhikAJE0nXa93YMrMrEXFA1P59b2AhcB5EbG5W18HpszMyig9wxR5MLOFwNURce1g5ZqZ2WiVDkwJuBxYFxGXNFe2mZnVVTowNRv4ILA6D2wG8Ll8X76ZmbWgdGDqdrpf4zczs5YUDUzlZVdI2ihpTZOFm5lZPT0P9JXA1BzgQOA0SQcOadYJTB0M/HdSYKrjKjxipZnZmCkamMrLbgN+2VC9ZmY2Sq0FpnpxYMrMrIxWAlN1ODBlZlZG8cCUmZmNraKBKTMzG3s9D/QRsRXoBKbWkWaJWivprE5oihcHplZKWtbpL2k+cCdwgKQNks5ofCvMzGxYRQNTedlpgxRoZmaDaSMwNWJfMzMrq2hgqmZfMzMrqHRgqmdfMzMrq3RgqnZfB6bMzMooHZiq3deBKTOzMkoHpmr1NTOzckoHpnr2NTOzsorOMDVc30LbYmZmXbQRmNqur5mZtaepwNSbJN0p6TlJ5w9Zdq6kNZLWSjqvobrNzKympgJTvwTOAb40pO9BwMdI99MfApwkaf8G6jYzs5qaCkxtjIilwG+G9H0zcFdEPJsHR7sVeG8DdZuZWU0lAlNVa4CjJE2WNBE4gRffbrmNA1NmZmU0GpjarlHEOuCLwCLgRmAVsHWYtg5MmZkVUOdAP1DoKSIuj4iZEXEU6Vr+j0dXopmZDaKRwNRIJL0y//t64H3A/H4KNTOz/jQSmJL0amAZsA/wQr6N8sCI2AwslDSZ9IfasyujXJqZWQuaCkz9C78dmnho37cOUqCZmQ2mjcDUJ3NYao2k+ZJ2b6p4MzPrrXRg6rX59VkRcRDp0s+pDdRtZmY1lQ5MQbo8tIek3YCJeJhiM7NWFQ1MRcSjpLP8R4DHgKci4qZubR2YMjMro2hgStLLSWf/+wGvAfaUdHq3tg5MmZmVUTowdRzwUERsiojfkCYnOXJ0JZqZ2SBKB6YeAY6QNFFpRpJjgXX9lWpmZv0oHZhaImkBsII0xs09wNwym2JmZt20EZi6ALhggBrNzGwARQNTkg6QtLLytdmzTJmZtavnGX0lMPUO0h9ml0q6PiLuqzTrBKbeU+0bEQ8AMyrf51HguiYKNzOzetoITHUcC/w0Ih7uu1ozMxu10jNMVZ3KCEMUOzBlZlZG0cDUtm+Qbss8GbhmuDYOTJmZlVF8hqlsDrAiIn4+yn5mZjag4jNMZafhmaXMzMZE8RmmJE0k3bHzx6U2wszMhtdGYOpZYPIANZqZ2QDamGFqkqQFku6XtE7SW5oq3szMeisamMouBW6MiFPyNf6JA1dtZma1FQ1MSdoHOAq4PLf7dUQ82UThZmZWT+nA1O8Am4ArJd0jaZ6kPbs1dGDKzKyM0oGp3YCZwGURcSjwDLDdNX5wYMrMrJTSgakNwIaIWJKfLyAd+M3MrCVFA1P5tsufSTogv3QscN8IXczMrGHFA1PAJ4Cr8w+JB4GPlNkUMzPrpo3A1EpgVv8lmpnZINoITK2XtDrPMLWsqcLNzKyeNgJTAG+PiMcHrNXMzPrQ5gxTZmY2BtqYYSqAmyQtl3TmcI0cmDIzK6ONGaZmR8RM0uQjZ0s6qlsjB6bMzMooPsNURPxz/ncjcB3pUpCZmbWkaGBK0p6S9u48Bt4JrOm3WDMzG72igSlgCnCdpM66vhkRNxbZEjMz66p0YGozcMggBZqZ2WCKB6by8l3zMMU3NFG0mZnV1/NAXwlMzSFdjjlN0oFDmnUCU18a5tucC6wboE4zM+tT8cCUpGnAicC8Buo1M7NRaiMw9WXg08ALIzVyYMrMrIyigSlJJwEbI2J5r7YOTJmZlVE6MDUbOFnSetIln2MkfWNUFZqZ2UBKzzD12YiYFhHTc79bIuL0vqs1M7NRa2OGKTMzG0PFZ5iqtFkMLB51hWZmNpCigSlJu0u6W9IqSWslXdRk8WZm1lvpGaaeA46JiC2SJgC3S/peRNzVSPVmZtZT0cBUJFvy0wn5azRj2ZuZ2YCKB6byODcrgY3AoohYMkw7B6bMzAooPsNURDwfETNIf6w9TNJBw7RzYMrMrIA6d90MNMNUR0Q8KWkxcDyefKQrXdTtZ2oz4gJfMTP716r0DFNTJU3Kj/cAjgPu77NWMzPrQ+kZpvYFvp7v3NkF+HZEeEx6M7MW1bqPnjTyZOSv5yEd4CuhqUmkP9i+FPgfediDzcATwJOku20EPNVY5WZmVkvp++i3Ap+KiBV5kvDlkhYN6WtmZgWVvo/+sYhYkR8/TZplajRj2ZuZ2YDamHgEAEnTgUOBrvfRm5lZGcXvoweQtBewEDhvuBEtHZgyMyuj9MQj5DFuFgJXR8S1w7VzYMrMrIzS99ELuBxYFxGX9F+mmZn1q/R99AcDHwRW5/FuAD6Xx7c3M7MWlJ545Ha6X+M3M7OWFJ14JC+7QtJGSR7fxsxsDPQ80FcCU3NIl2NOk3TgkGadwNSXunyLq0gDmZmZ2RgoGpjKy24j/SAwM7Mx0FpgqhffR29mVkYrgak6fB+9mVkZxQNTZmY2tooGpszMbOz1PNBHxFagE5haR5o8ZK2kszqhKUmvlrQB+M/A5yVtkLRPXjYfuBM4IL9+RqmNMTOz7ZUOTBERpw1SoJmZDaaNwNSIfc3MrKyigamafc3MrKDSgamefc3MrKzSganafR2YMjMro3RgqnZfB6bMzMooHZhy2MrMbIyVDkw5bGVmNsaKzjAVEZu79S20LWZm1kUbgant+pqZWXuaCkxJ0p/n5fdKmllZdq6kNZLW5jN9MzNrUVOBqTnA/vnrTOCy3Pcg4GOk++kPAU6StH9j1ZuZWU+NBKby87+J5C5gkqR9gTcDd0XEs3lwtFuB9zZYv5mZ9dBUYGq4NmuAoyRNljQROIEX3265jQNTZmZlNBWY6tomItYBXwQWATcCq4Ct3VbiwJSZWRlNBaaGbRMRl0fEzIg4ijT42Y/7L9fMzEarqcDU9cCH8t03RwBPRcRjAJJemf99PfA+YH5j1ZuZWU+NBKZI98mfAPwEeBb4SOVbLJQ0mTSy5dkR8UTD22BmZiNoKjAVwNnD9H3rIAWamdlg2ghMfTKHpdZImi9p9yY3wMzMRlY6MPVa0sxTsyLiINKln1Mbq97MzHoqHZiCdHloD0m7ARPxMMVmZq0qGpiKiEdJ88g+AjxGuhvnpm4rcWDKzKyMooEpSS8nne3vB7wG2FPS6d1W4sCUmVkZpQNTxwEPRcSmiPgNcC1wZP/lmpnZaJUOTD0CHCFpoiQBxwLrGqzfzMx6KBqYioglkhYAK0hj3NwDzC2xIWZm1l0bgakLgAsGqNHMzAZQNDAl6QBJKytfmz3LlJlZu3qe0VcCU+8g/dF1qaTrI+K+SrNqYOpwUmDq8Ih4AJhR+T6PAtc1uQFmZjayNgJTHccCP42Ihweu2szMais9w1TVqYwwRLEDU2ZmZRQNTG1bmG7LPBm4ZriVODBlZlZG8RmmsjnAioj4eT9FmplZ/4rPMJWdhmeWMjMbE8VnmJI0kXTHzh83X76ZmfXSRmDqWWDyADWamdkA2phhapKkBZLul7RO0lua3AAzMxtZ0RmmskuBGyPiTcAheFAzM7NWFQ1MSdoHOAq4HCAifh0RTzZXvpmZ9VI6MPU7wCbgSkn3SJonac9uK3FgysysjNKBqd2AmcBlEXEo8Ayw3TV+cGDKzKyU0oGpDcCGiFiSX19AOvCbmVlLigamIuJfgJ9JOiC3Oxa4DzMza03xwBTwCeDq/EPiwSHLzMyssDYCUyuBWf2XaGZmg2gjMLVe0uo8w9SyJos3M7Peis4wVVn+9oh4vLGqzcystjZnmDIzszHQxgxTAdwkabmkM4dbiQNTZmZltDHD1OyImEm6vHO2pKO6rcSBKTOzMorPMBURnX83AteRLgWZmVlLigamJO0paW+APMbNO4E1DdZvZmY9lA5MvQq4TlJnXd+MiBsb3wozMxtW0cBURDxIGoPezMzGSPHAVF6+ax6m+IamCjczs3ramGEK4Fw8s5SZ2ZgoHpiSNA04EZjXYN1mZlZTG4GpLwOfBl4YaSUOTJmZlVE0MCXpJGBjRCzvtRIHpszMyigdmJoNnCxpPemSzzGSvtF3tWZmNmqlZ5j6bERMi4jpud8tEXF6kxtgZmYja2OGKRvndFG3K3fNiQuGXik0s9EoPsNUpc1iYPGoKzQzs4EUDUxJ2l3S3ZJWSVor6aKmN8DMzEZWOjD1HHBMRBwCzACOz9fwzcysJUUDU/n5ltxmQv7yBVczsxYVD0zlcW5WAhuBRRGxpNtKHJgyMyujzh9jB5phKiKeB2ZImkQasvigiNhuTPqImAvMBZg1a5bP+q0VvmPI/jWoc6AfaIapjoh4UtJi4Hg8+YjZwPxDyuoqPcPU1Hwmj6Q9gOOA+5sr38zMeikdmNoX+Hq+c2cX4NsR4THpzcxaVHqGqXuBQwes0czGmZKXnXzJaXu1DvSSjgcuJZ3Rz4uIi4csV15+AumM/sMRsULS64C/AV5NGqZ4bkRc2mD9Zmat2hl/SJUOTG0FPhURbwaOAM7u0tfMzAoqHZh6LCJWAETE06TpBIfeg29mZgW1McMUAJKmk67XOzBlZtaiojNMbVso7QUsBM6LiM3dVuIZpszMyig9wxSSJpAO8ldHxLX9l2pmZv0oHZgScDmwLiIuabRyMzOrpXRgajbwQWB1HtgM4HP5vnwzM2tB6cDU7XS/fm9mZi0pOsNUXnaFpI2SPJCZmdkYKB2YAriKNGKlmZmNgaKBKYCIuA34ZZNFm5lZfa0FpnpxYMrMrIxWAlN1ODBlZlZG8cCUmZmNraKBqYZrNTOzPvQ80EfEVqATmFpHmiVqraSzOqEp0j32D5ICU18DPt7pL2k+cCdwgKQNks5oeBvMzGwERQNTedlpgxRoZmaDaSMwNWJfMzMrq2hgqmZfMzMrqHRgqk5fMzMrqM41+m5hqMNrtHltzb5ACkyRfhsA2CLpgRq1DWoK8HjdxrpwhxqfzbWPDdc+NmrXvrPWDQPX/obhFtQ50A8SmKodpIqIucDcGvU0RtKyiJjV5jqb4trHhmsfGztr7TtK3XUO9IMEpl5So6+ZmRVUOjBVp6+ZmRVUdIap4foW2ZL+tHqpqGGufWy49rGxs9a+Q9StlHUyM7PxqlZgyszMdl4+0JuZjXM+0NtORdJUSUsk3SPprTtAPSfvzEN7SLpQ0vktr3OSpI/3bjni9/iwpL9sqqYu3/+OGm3OkzSxVA15He9pYjQBH+iHyMM22I7rWOD+iDg0In7QxgolDXvTQkRcHxEXt1FHXSPVu4OYRGWE244d6f9eRBxZo9l5wKgO9H1s43tIw8cMZNwe6CWdLuluSSslfVXSrpIuy9MVrpV0UaXteklfkHQ78P78/CJJKyStlvSmMdqG70hanus9M792fK5rlaSb82uTJd2Uz3K/KulhSVMaWP90SfdLmidpjaSrJR0n6YeSfizpMEmvyHXeK+kuSQfnvhdKukLSYkkPSjqn8n27fTZnSPqzSpuPSbpkSD0zgP8FnJD7viHXMUXSLpJ+IOmdfWznf8vbuUjSfEnn57r/p6RbgXMlvavym8Q/SXpV7rvtzFLSVUqD+92Rt/mUft73mjV/KL/nqyT9bV73JZK+D3xx6Jl6/vym58d/ojTQ4D8BB1TavFHSjXmf+0HB/f5i4I35M1wq6fuSvgmsznVst9/n1z8i6Uf5M5ldeX2qpIX5ey2VNHu7NY6SpC3536PzvrAg7yNXKzkHeA3w/fyeI+mdku7M/z+vkbRXfn3o8WW4dhdLui9/rl+SdCRwMvC/83v1xr43KCLG3RfwZuAfgAn5+VeADwGvyM93BRYDB+fn64FPV/qvBz6RH38cmDdG29Gpdw9gDfAq0pAS+w1Z/ufAF/LjE0np4ykNrH86sBX4fdJJwXLgClLi+d3Ad4C/AC7I7Y8BVubHFwJ3AC8lxcB/AUwY4bPZE/hp5fU7gN/vUtOHgb+sPP8osAD4L8BX+9jGWcDK/B7vDfwYOD/vH1+ptHs5v71L7aPA/xlaD3AVcE1+rw4kjfNUYr/4N8ADnc8YeEVe9w3ArpX3//xKnzX58/y3pAPqRGAf0i3R5+c2NwP758eHA7cUqn86sCY/Php4prNPD7PfTwb2BR4BppKCmD+svO/fBP4wP349sK6BGrdU6nuKFPbchTS3Rmdd6yufwRTgNmDP/Pwz/Pb/5Hry8WW4dvkzfKCyj02q7FOnDLo9O/qveP06lrRDL5UEaYfZCPy7fIawG2nHORC4N/f51pDvcW3+dznwvtIFD+McSe/Nj19HGgvotoh4CCAifpmXHUWuMSK+K+mJBmt4KCI6Z1prgZsjIiStJv2HfQPwR3ndtyj9dvGy3Pe7EfEc8JykjaQfVF0/m4h4RtItwEmS1pEO+Kt7FRcR8yS9HzgLmNHH9v0h8PcR8au8jf9QWVbdJ6YB31IarO8lwEPDfL/vRMQLwH2ds/4CjgEWRMTjkPaD/F5eExHP9+j7VuC6iHgWQNL1+d+9gCOBa/L3gvRDug13d/bpbOh+vz/wamBxRGzK9X4L+L3c5jjgwErd+0jaOyKebrC+DXm9K0n7/e1D2hxBOp78MNfxEtIPhY5v9Wi3Gfh/wDxJ3yX90G7MeD3QC/h6RHx22wvSfsAi4A8i4glJVwG7V/o8M+R7PJf/fZ4xeJ8kHU3agd8SEc9KWgysovKr9hClAhHPVR6/UHn+Aul92TpCLdW+nfdxu8+mYh7wOeB+4EoASX9K+i2FiJgxtIPSH8Om5ad7AaP9zz3SKFLVfeIvgEsi4vr82Vw4TJ/qNpcaXUt0/7yr9W7lxZdmq/t6t767AE92e49bsK3uYfb7Tu3D7eO75Pa/KlRft/14KAGLYviJlp7p1U7SYaQToVNJs/od03fFQ4zXa/Q3A6dIeiWApFeQfqV7Bngqn2nNGcP66ngZ8ETe2d9EOhN4KfC2/EOrs12QfhX8QH5tDukyQ1uq6z4aeDwiNo/QfrvPRtIbACJiCekM7t8D8/NrfxIRM0Y4AH0RuJr06+/X+qj/duBdknbPZ7UnDtPuZcCj+fF/6GM9TbqZ9NvpZHjRflC1HpiZl88E9suv3wa8V9IekvYG3gWQP7OH8m9HncmEDilU/9Oky2TddNvvAZYAR+ffGCcA76/0uYl0YAS2/S2nDdXtuAuYLel3cw0TJf1elz5d2+V972WRZvM7j9/+djrSe1XbuDyjj4j7JH0euEnSLsBvSFMd3gOsJc1v+8MxLLGOG4GzJN1LunZ3F7CJdPnm2rxdG4F3ABcB8yWtAG4lXctsy4XAlbnOZ+lxEBzhs3k4N/k2MCMiel5+kvQ24A+A2RHxvKQ/kvSRiLiybvERsTRfvliVa1hGuiY71IWkyxqPkj6L/bq0aUWkIUj+FLhV0vOk/XqohaTxp1aSxpz6Ue67Il/2WEna3uqdSx8ALsufzwTS/BGrCtT/C6U/6K8BfgX8vLK4235PRDwm6ULSZY7HgBWkv7UBnAP8Ve6zG+mH2VmUNxf4nqTHIuLtkj5M+n/YueT1efL73hERm4Zp9zTw95J2J531fzIv+zvga0p//D0lIn7aT6EeAmEckrQemNW5hrszkXQD8GcRcXOL69wrIrbky0C3AWdGxIq21m9W2ni9dGM7GaUQzY+AX7V5kM/m5jPfFcBCH+RtvPEZvZnZOOczejOzcc4HejOzcc4HejOzcc4HejOzcc4HejOzce7/AxSYkWnRS+jmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_histogram_multilabel(all_topics, raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f7c3aa99394d00bac78ad80ca4f0c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f425e30c9974d9ca1e565ee6bc83a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27cc8e4e2fa0407aae8660b4270477ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322da2badc934bc384cde5abc530da89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12668 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfae96bf4f994cbfa9538217f8fc8bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42cb13400b6247a3900bd18b7e335e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5610 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = adjust_and_tokenize_datasets(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_path, \n",
    "                                  evaluation_strategy=\"epoch\", \n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  per_device_train_batch_size=8, \n",
    "                                  per_device_eval_batch_size=8, \n",
    "                                  num_train_epochs=6,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  logging_steps=1000,\n",
    "                                  load_best_model_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096', \n",
    "                                                            num_labels=num_labels, \n",
    "                                                            ignore_mismatched_sizes=True, \n",
    "                                                            problem_type='multi_label_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['eval'], \n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 12668\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9504\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9504' max='9504' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9504/9504 1:30:24, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.097200</td>\n",
       "      <td>0.051355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>0.043609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>0.049417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.049142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>0.049765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.051307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 189 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 169 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 387 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 191 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 241 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 497 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 484 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 214 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 474 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 167 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 224 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 214 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 276 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 243 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 367 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 237 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 479 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 335 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 170 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 204 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 235 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 258 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 171 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 386 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 318 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 377 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 208 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 248 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 224 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 220 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 331 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 193 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 183 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 332 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 254 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 190 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 281 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 243 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 213 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 210 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 280 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 218 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 170 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 503 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 165 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 249 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 176 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 377 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 193 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 254 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 173 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 255 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 361 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 203 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 361 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 191 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 233 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 188 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 185 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 244 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 194 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 176 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 181 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 209 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 479 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 200 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 262 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 447 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 357 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 248 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 173 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 219 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 331 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 339 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 371 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 475 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 240 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 357 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 243 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 219 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 187 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 137 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 144 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 371 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 337 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 166 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 446 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 159 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 191 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 255 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 321 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 232 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 195 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 218 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 367 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 480 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 510 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 174 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 435 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 168 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 226 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 174 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 186 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 439 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 254 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 211 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 243 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 241 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 221 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 190 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 370 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 475 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 339 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 418 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 187 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 446 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 497 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 226 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 154 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 247 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 210 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 446 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 216 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 318 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 211 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 337 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 267 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 226 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 184 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 249 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 323 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 179 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 337 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 511 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 281 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 203 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 197 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 324 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 224 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 183 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 208 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 357 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 185 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 486 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 214 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 222 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 236 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 174 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 159 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 201 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 227 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 232 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 386 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 185 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 183 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 216 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 189 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 218 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 370 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 387 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 249 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 240 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 211 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 208 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 209 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 241 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 226 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 186 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 262 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 249 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 247 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 8\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 208 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 241 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 141 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 195 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 222 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 207 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 213 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 263 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 174 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 200 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 235 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 186 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 199 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 259 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 511 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 187 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 168 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 254 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 175 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 172 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 201 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 183 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 173 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 162 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 178 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 337 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Saving model checkpoint to /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-1584\n",
      "Configuration saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-1584/config.json\n",
      "Model weights saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-1584/pytorch_model.bin\n",
      "tokenizer config file saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-1584/tokenizer_config.json\n",
      "Special tokens file saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-1584/special_tokens_map.json\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 180 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 199 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 226 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 247 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 258 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 181 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 203 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 474 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 497 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 479 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 209 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 335 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 209 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 194 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 446 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 235 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 188 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 332 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 141 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 206 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 221 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 207 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 203 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 255 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 197 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 254 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 204 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 258 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 174 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 176 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 216 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 255 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 447 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 200 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 198 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 281 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 240 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 205 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 367 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 171 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 371 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 371 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 188 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 332 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 247 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 178 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 476 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 219 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 318 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 167 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 250 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 207 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 232 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 339 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 239 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 503 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 281 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 245 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 165 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 503 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 193 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 475 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 174 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 237 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 188 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 497 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 190 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 225 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 168 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 203 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 174 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 276 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 224 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 387 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 484 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 387 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 206 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 179 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 214 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 186 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 243 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 377 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 209 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 245 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 244 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 225 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 168 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 418 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 337 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 386 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 357 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 227 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 240 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 276 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 332 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 248 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 201 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 205 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 175 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 178 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 371 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 388 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 249 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 196 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 195 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 194 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 339 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 172 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 224 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 486 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 497 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 418 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 200 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 240 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 387 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 200 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 446 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 179 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 198 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 367 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 324 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 191 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 377 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 206 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 332 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 267 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 185 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 263 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 511 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 164 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 480 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 248 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 198 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 158 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 189 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 435 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 475 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 194 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 237 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 157 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 370 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 178 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 188 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 203 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 224 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 213 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 226 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 181 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 480 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 171 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 244 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 225 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 206 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 232 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 236 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 366 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 172 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 339 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 233 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 173 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 233 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 213 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 139 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 337 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 337 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 157 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 262 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 331 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 201 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 244 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 348 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 162 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 318 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 386 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 232 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 193 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 439 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 280 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 388 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 240 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 206 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 367 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 475 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 181 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 249 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 239 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 276 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 263 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 473 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 206 to 512 to be a multiple of `config.attention_window`: 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 8\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 208 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 241 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 141 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 195 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 222 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 207 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 213 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 263 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 174 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 200 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 235 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 186 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 199 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 259 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 511 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 187 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 168 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 254 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 175 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 172 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 201 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 183 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 173 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 162 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 178 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 337 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Saving model checkpoint to /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-3168\n",
      "Configuration saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-3168/config.json\n",
      "Model weights saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-3168/pytorch_model.bin\n",
      "tokenizer config file saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-3168/tokenizer_config.json\n",
      "Special tokens file saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-3168/special_tokens_map.json\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 321 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 371 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 158 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 181 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 511 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 258 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 254 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 235 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 371 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 473 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 191 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 235 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 181 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 446 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 258 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 281 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 331 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 203 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 191 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 321 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 195 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 439 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 480 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 186 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 200 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 209 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 201 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 435 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 348 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 371 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 258 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 267 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 165 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 255 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 240 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 476 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 189 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 224 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 247 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 321 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 486 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 210 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 205 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 258 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 254 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 180 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 250 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 149 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 214 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 388 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 155 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 201 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 180 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 247 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 247 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 222 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 367 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 263 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 188 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 262 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 239 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 367 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 180 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 255 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 387 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 244 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 217 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 446 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 247 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 216 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 357 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 446 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 435 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 232 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 254 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 248 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 179 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 178 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 175 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 250 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 244 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 239 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 211 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 367 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 219 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 206 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 276 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 479 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 185 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 240 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 239 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 439 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 479 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 249 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 179 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 150 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 331 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 194 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 172 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 244 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 187 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 245 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 335 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 225 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 202 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 245 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 227 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 418 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 201 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 175 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 357 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 280 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 331 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 339 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 276 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 281 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 181 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 204 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 262 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 179 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 435 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 166 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 332 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 233 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 157 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 370 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 222 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 267 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 202 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 197 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 255 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 179 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 174 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 228 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 158 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 255 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 199 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 204 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 241 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 435 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 168 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 321 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 243 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 225 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 205 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 171 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 170 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 435 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 267 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 339 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 233 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 210 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 219 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 324 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 225 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 196 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 367 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 210 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 331 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 188 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 226 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 209 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 318 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 218 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 245 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 510 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 230 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 335 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 202 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 497 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 475 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 224 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 331 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 167 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 195 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 167 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 188 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 475 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 263 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 479 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 244 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 134 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 203 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 258 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 198 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 244 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 497 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 170 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 323 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 198 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 8\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 208 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 241 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 141 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 195 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 222 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 207 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 213 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 263 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 174 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 200 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 235 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 186 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 199 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 259 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 511 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 187 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 168 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 254 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 175 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 172 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 201 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 183 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 173 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 162 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 178 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 337 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Saving model checkpoint to /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-4752\n",
      "Configuration saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-4752/config.json\n",
      "Model weights saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-4752/pytorch_model.bin\n",
      "tokenizer config file saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-4752/tokenizer_config.json\n",
      "Special tokens file saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-4752/special_tokens_map.json\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 210 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 331 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 191 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 479 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 194 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 386 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 236 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 371 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 321 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 211 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 335 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 418 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 361 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 167 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 175 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 474 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 178 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 503 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 324 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 446 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 165 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 262 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 187 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 233 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 248 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 280 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 198 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 217 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 263 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 176 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 318 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 348 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 337 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 232 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 187 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 235 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 175 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 371 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 250 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 280 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 503 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 480 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 240 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 210 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 377 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 371 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 235 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 497 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 175 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 158 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 281 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 475 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 205 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 337 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 219 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 262 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 218 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 147 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 324 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 475 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 318 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 205 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 248 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 213 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 226 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 200 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 263 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 168 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 221 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 332 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 209 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 179 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 194 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 243 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 439 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 154 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 245 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 219 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 255 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 199 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 255 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 418 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 335 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 222 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 220 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 435 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 162 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 163 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 215 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 235 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 164 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 258 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 214 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 175 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 255 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 447 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 245 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 367 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 388 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 221 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 202 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 197 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 170 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 367 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 177 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 235 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 204 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 178 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 280 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 196 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 213 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 435 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 206 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 175 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 241 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 263 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 188 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 503 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 164 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 335 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 323 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 186 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 241 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 204 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 207 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 439 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 215 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 254 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 497 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 226 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 473 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 215 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 165 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 497 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 163 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 167 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 218 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 262 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 337 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 258 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 331 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 225 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 174 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 220 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 255 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 160 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 255 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 357 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 221 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 177 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 181 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 331 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 194 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 247 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 480 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 236 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 233 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 178 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 210 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 178 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 473 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 240 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 213 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 247 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 387 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 324 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 377 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 321 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 202 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 202 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 203 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 332 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 332 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 208 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 8\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 208 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 241 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 141 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 195 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 222 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 207 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 213 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 263 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 174 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 200 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 235 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 186 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 199 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 259 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 511 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 187 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 168 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 254 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 175 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 172 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 201 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 183 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 173 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 162 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 178 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 337 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Saving model checkpoint to /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-6336\n",
      "Configuration saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-6336/config.json\n",
      "Model weights saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-6336/pytorch_model.bin\n",
      "tokenizer config file saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-6336/tokenizer_config.json\n",
      "Special tokens file saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-6336/special_tokens_map.json\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 209 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 332 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 179 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 188 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 418 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 171 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 324 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 197 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 357 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 497 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 233 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 281 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 188 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 250 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 243 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 184 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 280 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 473 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 247 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 171 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 219 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 226 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 250 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 446 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 318 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 190 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 199 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 209 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 224 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 183 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 220 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 154 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 245 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 262 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 281 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 388 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 324 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 237 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 276 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 183 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 371 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 241 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 511 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 236 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 331 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 153 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 276 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 245 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 222 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 276 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 224 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 222 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 245 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 418 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 254 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 225 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 377 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 174 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 239 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 219 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 193 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 226 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 164 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 503 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 227 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 181 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 177 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 214 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 195 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 189 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 200 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 248 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 233 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 196 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 181 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 387 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 446 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 156 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 249 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 263 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 258 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 418 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 475 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 377 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 200 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 209 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 480 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 339 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 387 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 361 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 233 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 211 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 191 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 335 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 224 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 228 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 185 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 249 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 217 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 213 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 207 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 388 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 370 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 281 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 486 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 185 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 332 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 244 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 371 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 259 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 205 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 190 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 361 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 474 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 497 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 170 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 205 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 258 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 367 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 321 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 226 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 191 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 215 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 387 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 243 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 248 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 150 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 216 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 254 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 193 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 195 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 161 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 371 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 318 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 335 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 202 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 247 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 162 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 207 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 176 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 323 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 172 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 510 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 249 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 204 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 203 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 479 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 224 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 255 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 195 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 258 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 484 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 280 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 435 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 230 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 258 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 217 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 387 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 243 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 165 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 475 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 258 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 474 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 348 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 232 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 245 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 227 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 170 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 435 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 263 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 254 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 153 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 250 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 262 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 183 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 476 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 503 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 332 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 190 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 175 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 259 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 138 to 512 to be a multiple of `config.attention_window`: 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 8\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 208 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 241 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 141 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 195 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 222 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 207 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 213 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 263 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 174 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 200 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 235 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 186 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 199 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 259 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 511 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 187 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 168 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 254 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 175 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 172 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 201 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 183 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 173 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 162 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 178 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 337 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Saving model checkpoint to /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-7920\n",
      "Configuration saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-7920/config.json\n",
      "Model weights saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-7920/pytorch_model.bin\n",
      "tokenizer config file saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-7920/tokenizer_config.json\n",
      "Special tokens file saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-7920/special_tokens_map.json\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 446 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 172 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 215 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 263 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 181 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 435 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 222 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 154 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 213 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 248 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 497 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 224 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 213 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 167 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 210 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 331 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 213 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 259 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 195 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 219 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 197 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 474 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 245 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 179 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 267 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 173 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 259 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 214 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 321 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 247 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 258 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 497 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 175 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 197 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 371 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 337 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 324 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 181 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 188 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 276 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 321 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 280 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 189 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 215 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 243 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 165 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 190 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 245 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 179 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 371 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 178 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 244 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 321 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 217 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 191 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 175 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 323 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 259 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 215 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 232 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 217 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 174 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 224 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 179 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 215 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 280 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 203 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 153 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 361 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 267 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 222 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 224 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 176 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 198 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 387 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 335 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 255 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 224 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 267 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 237 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 263 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 332 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 418 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 475 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 184 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 324 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 185 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 226 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 233 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 281 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 510 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 484 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 207 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 188 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 366 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 221 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 217 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 213 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 244 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 318 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 281 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 447 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 473 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 203 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 247 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 479 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 323 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 236 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 194 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 243 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 166 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 151 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 180 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 205 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 225 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 170 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 473 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 208 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 318 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 198 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 377 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 280 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 239 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 267 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 203 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 377 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 168 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 386 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 217 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 170 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 418 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 198 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 324 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 228 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 237 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 254 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 245 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 214 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 262 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 258 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 435 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 171 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 222 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 370 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 240 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 174 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 480 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 233 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 175 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 318 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 232 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 172 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 361 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 475 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 243 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 497 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 357 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 196 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 335 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 214 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 237 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 247 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 218 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 220 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 435 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 475 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 171 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 280 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 503 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 335 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 263 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 179 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 318 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 474 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 418 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 387 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 480 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 337 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 474 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 175 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 446 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 237 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 247 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 221 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 198 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 377 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 214 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 324 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 195 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 165 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 202 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 188 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 248 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 237 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 198 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 213 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 166 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 235 to 512 to be a multiple of `config.attention_window`: 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 8\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 208 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 241 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 141 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 195 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 222 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 207 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 213 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 263 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 174 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 200 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 235 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 186 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 199 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 259 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 511 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 187 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 168 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 254 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 175 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 172 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 201 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 183 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 173 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 162 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 178 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 337 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Saving model checkpoint to /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-9504\n",
      "Configuration saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-9504/config.json\n",
      "Model weights saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-9504/pytorch_model.bin\n",
      "tokenizer config file saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-9504/tokenizer_config.json\n",
      "Special tokens file saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-9504/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/output/longformer-reuters-multilabel/checkpoint-3168 (score: 0.043609436601400375).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9504, training_loss=0.031389126924151925, metrics={'train_runtime': 5425.2437, 'train_samples_per_second': 14.01, 'train_steps_per_second': 1.752, 'total_flos': 2.0544021879015336e+16, 'train_loss': 0.031389126924151925, 'epoch': 6.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/saved-models/longformer-reuters-multilabel/config.json\n",
      "Model weights saved in /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/saved-models/longformer-reuters-multilabel/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/saved-models/longformer-reuters-multilabel/config.json\n",
      "Model config LongformerConfig {\n",
      "  \"_name_or_path\": \"allenai/longformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"LongformerForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\"\n",
      "  },\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/saved-models/longformer-reuters-multilabel/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing LongformerForSequenceClassification.\n",
      "\n",
      "All the weights of LongformerForSequenceClassification were initialized from the model checkpoint at /xdisk/bethard/kbozler/Documents/outside/reuters-article-classification/saved-models/longformer-reuters-multilabel.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "model = LongformerForSequenceClassification.from_pretrained(model_save_dir)\n",
    "model.config\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 5610\n",
      "  Batch size = 8\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 157 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='702' max='702' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [702/702 01:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 224 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 177 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 164 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 267 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 185 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 321 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 245 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 337 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 267 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 183 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 224 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 473 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 181 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 232 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 331 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 219 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 324 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 187 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 185 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 247 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 332 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 193 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 321 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 167 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 473 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 205 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 210 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 157 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 133 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 211 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 435 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 188 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 147 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 202 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 324 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 235 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 250 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 387 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 276 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 179 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 388 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 169 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 236 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 446 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 175 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 198 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 241 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 255 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 510 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 281 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 204 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 225 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 236 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 180 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 184 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 207 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 170 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 210 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 163 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 173 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 331 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 199 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 332 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 439 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 206 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 199 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 366 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 435 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 134 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 321 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 201 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 412 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 366 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 249 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 387 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 276 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 225 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 366 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 214 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 497 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 201 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 206 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 156 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 386 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 189 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 203 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 175 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 388 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 219 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 435 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 476 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 122 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 207 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 216 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 323 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 486 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 324 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 361 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 139 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 361 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 230 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 191 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 263 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 262 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 188 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 165 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 194 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 170 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 200 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 259 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 192 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 193 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 321 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 203 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 129 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 163 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 199 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 196 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 176 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 216 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 179 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 219 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 232 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 213 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 211 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 178 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 226 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 241 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 439 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 348 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 331 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.sigmoid(torch.tensor(predictions.predictions))\n",
    "threshold = torch.tensor([0.5])\n",
    "predicted_labels = (preds>threshold).float()*1\n",
    "target = torch.tensor(predictions.label_ids, dtype=torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = F1(num_classes=num_labels, average=None)\n",
    "precision = Precision(num_classes=num_labels, average=None)\n",
    "recall = Recall(num_classes=num_labels, average=None)\n",
    "#curve = PrecisionRecallCurve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7984, 0.8943, 0.6868, 0.8750, 0.8620, 0.7167, 0.7210])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1(predicted_labels, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6703, 0.8848, 0.5869, 0.9015, 0.7846, 0.6719, 0.7636])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision(predicted_labels, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9872, 0.9040, 0.8278, 0.8500, 0.9563, 0.7679, 0.6829])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall(predicted_labels, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interos-transformers",
   "language": "python",
   "name": "interos-transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
